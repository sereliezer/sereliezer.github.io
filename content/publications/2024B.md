---
title: "On Divergence Measures for Training GFlowNets"
date: 2024-09-27
categories: ["publication", "conference", "NeurIPS"]
topics: ["Deep Generative Models", "Statistical Divergences", "Variational Inference", "Generative Flow Networks"]
authors: ["Tiago da Silva", "Eliezer de Souza da Silva", "Diego Mesquita"]
---
{{< katex >}}

*Novel approach to training Generative Flow Networks (GFlowNets) by minimizing divergence measures such as Renyi-$\alpha$, Tsallis-$\alpha$, and Kullback-Leibler (KL) divergences. We develop efficient stochastic gradient estimators using variance reduction techniques, leading to faster and more stable training.*
<!--more-->
**Abstract** 

Generative Flow Networks (GFlowNets) are amortized inference models designed to sample from unnormalized distributions over composable objects, with applications in generative modeling for tasks in fields such as causal discovery, NLP, and drug discovery. Traditionally, the training procedure for GFlowNets seeks to minimize the expected log-squared difference between a proposal (forward policy) and a target (backward policy) distribution, which enforces certain flow-matching conditions. While this training procedure is closely related to variational inference (VI), directly attempting standard Kullback-Leibler (KL) divergence minimization can lead to proven biased and potentially high-variance estimators. Therefore, we first review four divergence measures, namely, Renyi-\\( \alpha \\)'s, Tsallis-\\( \alpha \\) 's, reverse and forward KL's, and design statistically efficient estimators for their stochastic gradients in the context of training GFlowNets. Then, we verify that properly minimizing these divergences yields a provably correct and empirically effective training scheme, often leading to significantly faster convergence than previously proposed optimization. To achieve this, we design control variates based on the REINFORCE leave-one-out and score-matching estimators to reduce the variance of the learning objectives' gradients. Our work contributes by narrowing the gap between GFlowNets training and generalized variational approximations, paving the way for algorithmic ideas informed by the divergence minimization viewpoint. 

**Publication**

* [Tiago da Silva](https://www.linkedin.com/in/tiagodasilvah/), Eliezer de Souza da Silva, and [Diego Mesquita](https://www.cs.helsinki.fi/u/aklami/). *On Divergence Measures for Training GFlowNets*. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). December, 2024. Vancouver, Canada. 

**Accepted at**
- [Neurips 2024 (to appear in)](https://openreview.net/forum?id=N5H4z0Pzvn)

**Extras**
[Bibtex](bibtex/bib.bib)

